{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"e0618dc194d63d51f1bb208bd0b3903adf60ba121b09f2eaeed74ef316cd2c88"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U datasets\n!pip install -q -U diffusers\n!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U ftfy\n!pip install -q -U huggingface-hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport math\n\nfrom datasets import load_dataset, Image\nfrom typing import Optional\n\nfrom diffusers import AutoencoderKL, DDPMScheduler, LMSDiscreteScheduler, StableDiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers.optimization import get_scheduler\n\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torch import autocast","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:29:23.801412Z","iopub.execute_input":"2022-12-15T02:29:23.801819Z","iopub.status.idle":"2022-12-15T02:29:23.808549Z","shell.execute_reply.started":"2022-12-15T02:29:23.801785Z","shell.execute_reply":"2022-12-15T02:29:23.807345Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"class Upsample2D(torch.nn.Module):\n    def __init__(self, channels, use_conv=False, out_channels=None, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.name = name\n        \n        if use_conv:\n            self.conv = torch.nn.Conv2d(self.channels, self.out_channels, 3, padding=1)\n\n    def forward(self, hidden_states, output_size):\n        if output_size is None:\n            hidden_states = torch.nn.functional.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n        else:\n            hidden_states = torch.nn.functional.interpolate(hidden_states, size=output_size, mode=\"nearest\")\n            \n        if self.use_conv:\n            hidden_states = self.conv(hidden_states)\n        return hidden_states\n\nclass Downsample2D(torch.nn.Module):\n    def __init__(self, channels, use_conv=False, out_channels=None, name=\"conv\"):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.name = name\n\n        if use_conv:\n            self.conv = torch.nn.Conv2d(self.channels, self.out_channels, 3, stride=2, padding=1)\n        else:\n            self.conv = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, hidden_states):\n        hidden_states = self.conv(hidden_states)\n        return hidden_states","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:54.763154Z","iopub.execute_input":"2022-12-14T23:54:54.764237Z","iopub.status.idle":"2022-12-14T23:54:54.775590Z","shell.execute_reply.started":"2022-12-14T23:54:54.764195Z","shell.execute_reply":"2022-12-14T23:54:54.774738Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class DownBlock2D(torch.nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, temb_channels: int, num_layers: int = 1, add_downsample=True):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels))\n\n        self.resnets = torch.nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsampler = Downsample2D(out_channels, use_conv=True, out_channels=out_channels)\n        else:\n            self.downsampler = None\n\n    def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb)\n            output_states += (hidden_states,)\n\n        if self.downsampler:\n            hidden_states = self.downsampler(hidden_states)\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states\n\n\nclass UpBlock2D(torch.nn.Module):\n    def __init__(self, in_channels: int, prev_output_channel: int, out_channels: int, temb_channels: int, num_layers: int = 1, add_upsample=True):\n        super().__init__()\n        self.add_upsample = add_upsample\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels))\n\n        self.resnets = torch.nn.ModuleList(resnets)\n\n        if self.add_upsample:\n            self.upsampler = Upsample2D(out_channels, use_conv=True, out_channels=out_channels)\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):\n        for resnet in self.resnets:\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n            hidden_states = resnet(hidden_states, temb)\n\n        if self.add_upsample:\n            hidden_states = self.upsampler(hidden_states, upsample_size)\n\n        return hidden_states","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:55.649544Z","iopub.execute_input":"2022-12-14T23:54:55.650444Z","iopub.status.idle":"2022-12-14T23:54:55.664550Z","shell.execute_reply.started":"2022-12-14T23:54:55.650392Z","shell.execute_reply":"2022-12-14T23:54:55.663575Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class ResnetBlock2D(torch.nn.Module):\n    def __init__(self, in_channels, out_channels=None, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = in_channels if out_channels is None else out_channels\n        \n        self.nonlinearity = torch.nn.SiLU()\n        self.norm1 = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-5, affine=True)\n        self.conv1 = torch.nn.Conv2d(in_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n\n        self.time_emb_proj = torch.nn.Linear(temb_channels, self.out_channels)\n\n        self.norm2 = torch.nn.GroupNorm(num_groups=32, num_channels=self.out_channels, eps=1e-5, affine=True)\n        self.conv2 = torch.nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n\n        self.use_conv_shortcut = self.in_channels != self.out_channels\n        if self.use_conv_shortcut:\n            self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, input_tensor, temb):\n        hidden_states = input_tensor\n        hidden_states = self.norm1(hidden_states)\n        hidden_states = self.nonlinearity(hidden_states)\n        hidden_states = self.conv1(hidden_states)\n        \n        if temb is not None:\n            temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n            hidden_states = hidden_states + temb\n\n        hidden_states = self.norm2(hidden_states)\n        hidden_states = self.nonlinearity(hidden_states)\n        hidden_states = self.conv2(hidden_states)\n\n        if self.use_conv_shortcut:\n            input_tensor = self.conv_shortcut(input_tensor)\n\n        output_tensor = (input_tensor + hidden_states)\n        return output_tensor","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:55.906653Z","iopub.execute_input":"2022-12-14T23:54:55.907389Z","iopub.status.idle":"2022-12-14T23:54:55.919655Z","shell.execute_reply.started":"2022-12-14T23:54:55.907350Z","shell.execute_reply":"2022-12-14T23:54:55.918664Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CrossAttention(torch.nn.Module):\n    def __init__(self, query_dim: int, heads: int = 8, dim_head: int = 64, cross_attention_dim=None):\n        super().__init__()\n        inner_dim = dim_head * heads\n        cross_attention_dim = cross_attention_dim if cross_attention_dim is not None else query_dim\n\n        self.scale = dim_head**-0.5\n        self.heads = heads\n\n        self.to_q = torch.nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = torch.nn.Linear(cross_attention_dim, inner_dim, bias=False)\n        self.to_v = torch.nn.Linear(cross_attention_dim, inner_dim, bias=False)\n\n        self.to_out = torch.nn.Linear(inner_dim, query_dim)\n\n    def reshape_heads_to_batch_dim(self, tensor):\n        batch_size, seq_len, dim = tensor.shape\n        head_size = self.heads\n        tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)\n        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size * head_size, seq_len, dim // head_size)\n        return tensor\n\n    def reshape_batch_dim_to_heads(self, tensor):\n        batch_size, seq_len, dim = tensor.shape\n        head_size = self.heads\n        tensor = tensor.reshape(batch_size // head_size, head_size, seq_len, dim)\n        tensor = tensor.permute(0, 2, 1, 3).reshape(batch_size // head_size, seq_len, dim * head_size)\n        return tensor\n\n    def forward(self, hidden_states, context=None, mask=None):\n        batch_size, sequence_length, _ = hidden_states.shape\n\n        query = self.to_q(hidden_states)\n        context = context if context is not None else hidden_states\n        key = self.to_k(context)\n        value = self.to_v(context)\n\n\n        dim = query.shape[-1]\n\n        query = self.reshape_heads_to_batch_dim(query)\n        key = self.reshape_heads_to_batch_dim(key)\n        value = self.reshape_heads_to_batch_dim(value)\n\n        # attention\n        hidden_states = self._attention(query, key, value)\n        \n        hidden_states = self.to_out(hidden_states)\n        return hidden_states\n\n    def _attention(self, query, key, value):\n        attention_scores = torch.baddbmm(\n            torch.empty(query.shape[0], query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n            query,\n            key.transpose(-1, -2),\n            beta=0,\n            alpha=self.scale,\n        )\n        attention_probs = attention_scores.softmax(dim=-1)\n\n        # cast back to the original dtype\n        attention_probs = attention_probs.to(value.dtype)\n\n        # compute attention output\n        hidden_states = torch.bmm(attention_probs, value)\n\n        # reshape hidden_states\n        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n        return hidden_states","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:56.108930Z","iopub.execute_input":"2022-12-14T23:54:56.109259Z","iopub.status.idle":"2022-12-14T23:54:56.123480Z","shell.execute_reply.started":"2022-12-14T23:54:56.109228Z","shell.execute_reply":"2022-12-14T23:54:56.122407Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class FeedForward(torch.nn.Module):\n    def __init__(self, dim: int, dim_out: Optional[int] = None, mult: int = 4):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = dim_out if dim_out is not None else dim\n        self.act_fn = GEGLU(dim, inner_dim)\n        self.linear = torch.nn.Linear(inner_dim, dim_out)\n\n    def forward(self, hidden_states):\n        hidden_states = self.act_fn(hidden_states)\n        hidden_states = self.linear(hidden_states)\n        return hidden_states\n\n\nclass GEGLU(torch.nn.Module):\n    def __init__(self, dim_in: int, dim_out: int):\n        super().__init__()\n        self.proj = torch.nn.Linear(dim_in, dim_out * 2)\n\n    def gelu(self, gate):\n        return torch.nn.functional.gelu(gate.to(dtype=torch.float32)).to(dtype=gate.dtype)\n\n    def forward(self, hidden_states):\n        hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)\n        return hidden_states * self.gelu(gate)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:56.261964Z","iopub.execute_input":"2022-12-14T23:54:56.262836Z","iopub.status.idle":"2022-12-14T23:54:56.272724Z","shell.execute_reply.started":"2022-12-14T23:54:56.262789Z","shell.execute_reply":"2022-12-14T23:54:56.271608Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class BasicTransformerBlock(torch.nn.Module):\n    def __init__(self, dim: int, num_attention_heads: int, attention_head_dim: int):\n        super().__init__()\n        # 1. Self-Attn\n        self.attn1 = CrossAttention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim)\n        self.norm1 = torch.nn.LayerNorm(dim)\n\n        # 2. Cross-Attn\n        self.attn2 = CrossAttention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, cross_attention_dim=768)  \n        self.norm2 = torch.nn.LayerNorm(dim)\n\n        # 3. Feed-forward\n        self.ff = FeedForward(dim)\n        self.norm3 = torch.nn.LayerNorm(dim)\n\n    def forward(self, hidden_states, context=None):\n        # 1. Self-Attention\n        norm_hidden_states = self.norm1(hidden_states)\n        hidden_states = self.attn1(norm_hidden_states) + hidden_states\n\n        # 2. Cross-Attention\n        norm_hidden_states = self.norm2(hidden_states)\n        hidden_states = self.attn2(norm_hidden_states, context=context) + hidden_states\n\n        # 3. Feed-forward\n        hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n        return hidden_states","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:56.454150Z","iopub.execute_input":"2022-12-14T23:54:56.455959Z","iopub.status.idle":"2022-12-14T23:54:56.464986Z","shell.execute_reply.started":"2022-12-14T23:54:56.455912Z","shell.execute_reply":"2022-12-14T23:54:56.463696Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Transformer2DModel(torch.nn.Module):\n    def __init__(self, num_attention_heads: int = 16, attention_head_dim: int = 88, in_channels: Optional[int] = None):\n        super().__init__()\n        self.num_attention_heads = num_attention_heads\n        self.attention_head_dim = attention_head_dim\n        inner_dim = num_attention_heads * attention_head_dim\n\n        self.in_channels = in_channels\n\n        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n        self.proj_in = torch.nn.Conv2d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n\n        # 3. Define transformers blocks\n        self.transformer_block = BasicTransformerBlock(inner_dim, num_attention_heads, attention_head_dim)\n\n        # 4. Define output layers\n        self.proj_out = torch.nn.Conv2d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0)\n\n\n    def forward(self, hidden_states, encoder_hidden_states=None):\n        # 1. Input\n        batch, channel, height, weight = hidden_states.shape\n        residual = hidden_states\n\n        hidden_states = self.norm(hidden_states)\n\n        hidden_states = self.proj_in(hidden_states)\n\n        inner_dim = hidden_states.shape[1]\n        hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch, height * weight, inner_dim)\n\n\n        # 2. Blocks\n        hidden_states = self.transformer_block(hidden_states, context=encoder_hidden_states)\n\n\n        # 3. Output\n        hidden_states = (hidden_states.reshape(batch, height, weight, inner_dim).permute(0, 3, 1, 2).contiguous())\n\n\n        hidden_states = self.proj_out(hidden_states)    \n\n        output = hidden_states + residual\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:56.625300Z","iopub.execute_input":"2022-12-14T23:54:56.625641Z","iopub.status.idle":"2022-12-14T23:54:56.636499Z","shell.execute_reply.started":"2022-12-14T23:54:56.625611Z","shell.execute_reply":"2022-12-14T23:54:56.635527Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class CrossAttnDownBlock2D(torch.nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, temb_channels: int, num_layers: int = 1, add_downsample=True):\n        super().__init__()\n        resnets = []\n        attentions = []\n\n        self.has_cross_attention = True\n        self.attn_num_head_channels = 8\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels))\n            attentions.append(Transformer2DModel(num_attention_heads=self.attn_num_head_channels, \n                                                 attention_head_dim=(out_channels // self.attn_num_head_channels), \n                                                 in_channels=out_channels))\n           \n        self.attentions = torch.nn.ModuleList(attentions)\n        self.resnets = torch.nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsampler = Downsample2D(out_channels, use_conv=True, out_channels=out_channels)\n        else:\n            self.downsampler = None\n\n    def forward(self, hidden_states, temb=None, encoder_hidden_states=None):\n        output_states = ()\n        for i, (resnet, attn) in enumerate(zip(self.resnets, self.attentions)):\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states)\n            output_states += (hidden_states,)\n\n\n        if self.downsampler is not None:\n            hidden_states = self.downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n        \n        return hidden_states, output_states","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:56.781448Z","iopub.execute_input":"2022-12-14T23:54:56.782094Z","iopub.status.idle":"2022-12-14T23:54:56.792885Z","shell.execute_reply.started":"2022-12-14T23:54:56.782050Z","shell.execute_reply":"2022-12-14T23:54:56.791891Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CrossAttnUpBlock2D(torch.nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, prev_output_channel: int, temb_channels: int, num_layers: int = 1, add_upsample=True):\n        super().__init__()\n        resnets = []\n        attentions = []\n        self.has_cross_attention = True\n        self.attn_num_head_channels = 8\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels))\n            attentions.append(Transformer2DModel(self.attn_num_head_channels, out_channels // self.attn_num_head_channels, in_channels=out_channels))\n            \n        self.attentions = torch.nn.ModuleList(attentions)\n        self.resnets = torch.nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsampler = Upsample2D(out_channels, use_conv=True, out_channels=out_channels)\n        else:\n            self.upsampler = None\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, encoder_hidden_states=None, upsample_size=None):\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            # print(hidden_states.shape, res_hidden_states.shape)\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states)\n\n        if self.upsampler is not None:\n            hidden_states = self.upsampler(hidden_states, upsample_size)\n\n        return hidden_states","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:57.011755Z","iopub.execute_input":"2022-12-14T23:54:57.012971Z","iopub.status.idle":"2022-12-14T23:54:57.024071Z","shell.execute_reply.started":"2022-12-14T23:54:57.012922Z","shell.execute_reply":"2022-12-14T23:54:57.022929Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_timestep_embedding(timesteps: torch.Tensor, embedding_dim: int, downscale_freq_shift: float = 1, scale: float = 1, max_period: int = 10000):\n    assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n\n    half_dim = embedding_dim // 2\n    exponent = -math.log(max_period) * torch.arange(\n        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n    )\n    exponent = exponent / (half_dim - downscale_freq_shift)\n\n    emb = torch.exp(exponent)\n    emb = timesteps[:, None].float() * emb[None, :]\n\n    # scale embeddings\n    emb = scale * emb\n\n    # concat sine and cosine embeddings\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n\n    # flip sine and cosine embeddings\n    emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n\n    # zero pad\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:57.211959Z","iopub.execute_input":"2022-12-14T23:54:57.212596Z","iopub.status.idle":"2022-12-14T23:54:57.221571Z","shell.execute_reply.started":"2022-12-14T23:54:57.212554Z","shell.execute_reply":"2022-12-14T23:54:57.220432Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Timesteps(torch.nn.Module):\n    def __init__(self, num_channels: int, downscale_freq_shift: float):\n        super().__init__()\n        self.num_channels = num_channels\n        self.downscale_freq_shift = downscale_freq_shift\n\n    def forward(self, timesteps):\n        t_emb = get_timestep_embedding(\n            timesteps,\n            self.num_channels,\n            downscale_freq_shift=self.downscale_freq_shift,\n        )\n        return t_emb","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:57.441531Z","iopub.execute_input":"2022-12-14T23:54:57.441891Z","iopub.status.idle":"2022-12-14T23:54:57.448454Z","shell.execute_reply.started":"2022-12-14T23:54:57.441860Z","shell.execute_reply":"2022-12-14T23:54:57.447380Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class TimestepEmbedding(torch.nn.Module):\n    def __init__(self, in_channels: int, time_embed_dim: int, out_dim: int = None):\n        super().__init__()\n\n        self.linear_1 = torch.nn.Linear(in_channels, time_embed_dim)\n        self.act = torch.nn.SiLU()\n\n        if out_dim is not None:\n            time_embed_dim_out = out_dim\n        else:\n            time_embed_dim_out = time_embed_dim\n        self.linear_2 = torch.nn.Linear(time_embed_dim, time_embed_dim_out)\n\n    def forward(self, sample):\n        sample = self.linear_1(sample)\n\n        if self.act is not None:\n            sample = self.act(sample)\n\n        sample = self.linear_2(sample)\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:57.670262Z","iopub.execute_input":"2022-12-14T23:54:57.670592Z","iopub.status.idle":"2022-12-14T23:54:57.678424Z","shell.execute_reply.started":"2022-12-14T23:54:57.670564Z","shell.execute_reply":"2022-12-14T23:54:57.677209Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_down_block(down_block_type, **kwargs):\n    if down_block_type == \"DownBlock2D\":\n        return DownBlock2D(**kwargs)\n    elif down_block_type == \"CrossAttnDownBlock2D\":\n        return CrossAttnDownBlock2D(**kwargs)\n    else:\n        raise ValueError(f\"{down_block_type} does not exist.\")\n\ndef get_up_block(up_block_type, **kwargs):\n    if up_block_type == \"UpBlock2D\":\n        return UpBlock2D(**kwargs)\n    elif up_block_type == \"CrossAttnUpBlock2D\":\n        return CrossAttnUpBlock2D(**kwargs)\n    else:\n        raise ValueError(f\"{get_up_block} does not exist.\")","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:57.888746Z","iopub.execute_input":"2022-12-14T23:54:57.889412Z","iopub.status.idle":"2022-12-14T23:54:57.895610Z","shell.execute_reply.started":"2022-12-14T23:54:57.889373Z","shell.execute_reply":"2022-12-14T23:54:57.894651Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class UNetMidBlock2DCrossAttn(torch.nn.Module):\n    def __init__(self, in_channels: int, temb_channels: int):\n        super().__init__()\n        self.has_cross_attention = True\n        self.attn_num_head_channels = 8\n        self.resnet_1 = ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels)\n        self.resnet_2 = ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels)\n        self.attention = Transformer2DModel(self.attn_num_head_channels, in_channels // self.attn_num_head_channels, in_channels=in_channels)\n\n    def forward(self, hidden_states, temb=None, encoder_hidden_states=None):\n        hidden_states = self.resnet_1(hidden_states, temb)\n        hidden_states = self.attention(hidden_states, encoder_hidden_states)\n        hidden_states = self.resnet_2(hidden_states, temb)\n        return hidden_states","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:58.126387Z","iopub.execute_input":"2022-12-14T23:54:58.127112Z","iopub.status.idle":"2022-12-14T23:54:58.135781Z","shell.execute_reply.started":"2022-12-14T23:54:58.127067Z","shell.execute_reply":"2022-12-14T23:54:58.134761Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class UNet2DConditionModel(torch.nn.Module):\n    def __init__(self, sample_size: Optional[int] = None, in_channels: int = 4, out_channels: int = 4, freq_shift: int = 0, layers_per_block: int = 2):\n        super().__init__()\n        self.down_block_types = (\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\")\n        self.up_block_types = (\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\")\n        self.block_out_channels = (320, 640, 1280)\n\n        self.sample_size = sample_size\n        time_embed_dim = self.block_out_channels[0] * 3\n\n        # input\n        self.conv_in = torch.nn.Conv2d(in_channels, self.block_out_channels[0], kernel_size=3, padding=(1, 1))\n\n        # time\n        self.time_proj = Timesteps(self.block_out_channels[0], freq_shift)\n        timestep_input_dim = self.block_out_channels[0]\n\n        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)\n\n        self.down_blocks = torch.nn.ModuleList([])\n        self.mid_block = None\n        self.up_blocks = torch.nn.ModuleList([])\n\n        attention_head_dim = (8,) * len(self.down_block_types)\n\n        # down\n        output_channel = self.block_out_channels[0]\n        for i, down_block_type in enumerate(self.down_block_types):\n            input_channel = output_channel\n            output_channel = self.block_out_channels[i]\n            is_final_block = i == len(self.block_out_channels) - 1\n            down_block = get_down_block(down_block_type, \n                                        num_layers=layers_per_block, \n                                        in_channels=input_channel, \n                                        out_channels=output_channel, \n                                        temb_channels=time_embed_dim, \n                                        add_downsample=not is_final_block)\n            self.down_blocks.append(down_block)\n\n        # mid\n        self.mid_block = UNetMidBlock2DCrossAttn(in_channels=self.block_out_channels[-1], temb_channels=time_embed_dim)\n\n        # count how many layers upsample the images\n        self.num_upsamplers = 0\n\n        # up\n        reversed_block_out_channels = list(reversed(self.block_out_channels))\n        reversed_attention_head_dim = list(reversed(attention_head_dim))\n        output_channel = reversed_block_out_channels[0]\n        for i, up_block_type in enumerate(self.up_block_types):\n            is_final_block = i == len(self.block_out_channels) - 1\n\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n            input_channel = reversed_block_out_channels[min(i + 1, len(self.block_out_channels) - 1)]\n\n            # add upsample block for all BUT final layer\n            if not is_final_block:\n                add_upsample = True\n                self.num_upsamplers += 1\n            else:\n                add_upsample = False\n\n            up_block = get_up_block(up_block_type, \n                                    num_layers=layers_per_block + 1, \n                                    in_channels=input_channel, \n                                    out_channels=output_channel, \n                                    prev_output_channel=prev_output_channel, \n                                    temb_channels=time_embed_dim, \n                                    add_upsample=add_upsample)\n\n            self.up_blocks.append(up_block)\n            prev_output_channel = output_channel\n\n        # out\n        self.conv_norm_out = torch.nn.GroupNorm(num_channels=self.block_out_channels[0], num_groups=32, eps=1e-5)\n        self.conv_act = torch.nn.SiLU()\n        self.conv_out = torch.nn.Conv2d(self.block_out_channels[0], out_channels, kernel_size=3, padding=1)\n\n    def forward(self, sample, timestep, encoder_hidden_states):\n        # By default samples have to be AT least a multiple of the overall upsampling factor.\n        # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n        # However, the upsampling interpolation output size can be forced to fit any upsampling size\n        # on the fly if necessary.\n        default_overall_up_factor = 2**self.num_upsamplers\n\n        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n        forward_upsample_size = False\n        upsample_size = None\n\n        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n            forward_upsample_size = True\n\n        # 1. time\n        timesteps = timestep\n        if not torch.is_tensor(timesteps):\n            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n            # This would be a good case for the `match` statement (Python 3.10+)\n            if isinstance(timestep, float):\n                dtype = torch.float32\n            else:\n                dtype = torch.int32\n            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n        elif len(timesteps.shape) == 0:\n            timesteps = timesteps[None].to(sample.device)\n\n        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n        timesteps = timesteps.expand(sample.shape[0])\n\n        t_emb = self.time_proj(timesteps)\n\n        # timesteps does not contain any weights and will always return f32 tensors\n        # but time_embedding might actually be running in fp16. so we need to cast here.\n        # there might be better ways to encapsulate this.\n        # t_emb = t_emb.to(dtype=self.dtype)\n        emb = self.time_embedding(t_emb)\n\n        # 2. pre-process\n        sample = self.conv_in(sample)\n\n        # 3. down\n        down_block_res_samples = (sample,)\n        for i, downsample_block in enumerate(self.down_blocks):\n            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n                sample, res_samples = downsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    encoder_hidden_states=encoder_hidden_states,\n                )\n            else:\n                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n\n            down_block_res_samples += res_samples\n\n        # 4. mid\n        sample = self.mid_block(sample, emb, encoder_hidden_states=encoder_hidden_states)\n\n        # 5. up\n        for i, upsample_block in enumerate(self.up_blocks):\n            is_final_block = i == len(self.up_blocks) - 1\n\n            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n\n            # if we have not reached the final block and need to forward the\n            # upsample size, we do it here\n            if not is_final_block and forward_upsample_size:\n                upsample_size = down_block_res_samples[-1].shape[2:]\n\n            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n                sample = upsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    res_hidden_states_tuple=res_samples,\n                    encoder_hidden_states=encoder_hidden_states,\n                    upsample_size=upsample_size,\n                )\n            else:\n                sample = upsample_block(\n                    hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n                )\n\n\n        # 6. post-process\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_act(sample)\n        sample = self.conv_out(sample)\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:58.381212Z","iopub.execute_input":"2022-12-14T23:54:58.381593Z","iopub.status.idle":"2022-12-14T23:54:58.413633Z","shell.execute_reply.started":"2022-12-14T23:54:58.381559Z","shell.execute_reply":"2022-12-14T23:54:58.412476Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfFolder\n\nHfFolder.save_token('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')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:54:58.593661Z","iopub.execute_input":"2022-12-14T23:54:58.594370Z","iopub.status.idle":"2022-12-14T23:54:58.600277Z","shell.execute_reply.started":"2022-12-14T23:54:58.594333Z","shell.execute_reply":"2022-12-14T23:54:58.599118Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:55:00.763448Z","iopub.execute_input":"2022-12-14T23:55:00.763982Z","iopub.status.idle":"2022-12-14T23:55:00.769774Z","shell.execute_reply.started":"2022-12-14T23:55:00.763945Z","shell.execute_reply":"2022-12-14T23:55:00.768731Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# load pre-trained models \ntokenizer = CLIPTokenizer.from_pretrained('CompVis/stable-diffusion-v1-4', subfolder=\"tokenizer\")\ntext_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14').to(device)\nvae = AutoencoderKL.from_pretrained('CompVis/stable-diffusion-v1-4', subfolder=\"vae\").to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:55:12.540067Z","iopub.execute_input":"2022-12-14T23:55:12.540441Z","iopub.status.idle":"2022-12-14T23:56:11.335106Z","shell.execute_reply.started":"2022-12-14T23:55:12.540408Z","shell.execute_reply":"2022-12-14T23:56:11.334066Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c3c7fc7606c429e89c739d9a306462f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd200532e93c491698e1b82aef97a1d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f65bce3db94a4d81be6217d76cf8d79b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625a3c1abbd84736a6021a9134b2a121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b4b80d009e04594b80de086ca999e41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ff1dfa1fe14d5a8c231817ae1c2f3c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias']\n- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81899c91cf7f4afc9cc98c64ccf0314d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/522 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a70c8796eef494f945631687b4225b6"}},"metadata":{}}]},{"cell_type":"code","source":"# load UNet\nunet = UNet2DConditionModel().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:57:45.089913Z","iopub.execute_input":"2022-12-14T23:57:45.090362Z","iopub.status.idle":"2022-12-14T23:57:49.623825Z","shell.execute_reply.started":"2022-12-14T23:57:45.090326Z","shell.execute_reply":"2022-12-14T23:57:49.622729Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Freeze vae and text_encoder\nvae.requires_grad_(False)\ntext_encoder.requires_grad_(False)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:58:28.412153Z","iopub.execute_input":"2022-12-14T23:58:28.412574Z","iopub.status.idle":"2022-12-14T23:58:28.428018Z","shell.execute_reply.started":"2022-12-14T23:58:28.412540Z","shell.execute_reply":"2022-12-14T23:58:28.426951Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CLIPTextModel(\n  (text_model): CLIPTextTransformer(\n    (embeddings): CLIPTextEmbeddings(\n      (token_embedding): Embedding(49408, 768)\n      (position_embedding): Embedding(77, 768)\n    )\n    (encoder): CLIPEncoder(\n      (layers): ModuleList(\n        (0): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): CLIPEncoderLayer(\n          (self_attn): CLIPAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): CLIPMLP(\n            (activation_fn): QuickGELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":" optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4, weight_decay=1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:58:32.260665Z","iopub.execute_input":"2022-12-14T23:58:32.261053Z","iopub.status.idle":"2022-12-14T23:58:32.268886Z","shell.execute_reply.started":"2022-12-14T23:58:32.261020Z","shell.execute_reply":"2022-12-14T23:58:32.267475Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### UNet\n\n> The UNet model is tasked with predicting (recovering) the noise that has corrupted the latent image.  \n\n> UNet is inspired by medical tasks (segmentation specifically) and consists three important parts:\n \n 1. Downsampling Subnetwork\n 2. Intermediate or Middle Subnetwork\n 3. Upsampling Subnetwork\n\n > The Downsampling Subnetwork consists of Residual Blocks + Transformer Encoder + Downsampling Layer\n\n > The Upsampling Subnetwork consists of Residual Blocks + Transformer Encoder + Upsampling Layer\n \n > The Intermediate or Middle Subnetwork consists of Residual Blocks + Transformer Encoder\n\n\nFunction of each block????","metadata":{}},{"cell_type":"markdown","source":"### Blocks\n\nThis is quite an involved network, so we will go about creating each block step by step. Let us first create the residual blocks, since they come first\n","metadata":{}},{"cell_type":"markdown","source":"#### ResidualBlock\n\nTo implement the ResidualBlock, we need to implement the downsampling and upsampling blocks","metadata":{}},{"cell_type":"code","source":"noise_scheduler = DDPMScheduler.from_pretrained('CompVis/stable-diffusion-v1-4', subfolder=\"scheduler\")","metadata":{"execution":{"iopub.status.busy":"2022-12-14T23:59:21.199324Z","iopub.execute_input":"2022-12-14T23:59:21.199722Z","iopub.status.idle":"2022-12-14T23:59:21.911997Z","shell.execute_reply.started":"2022-12-14T23:59:21.199668Z","shell.execute_reply":"2022-12-14T23:59:21.911054Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"891f73f6ed35464f8e538282658b13ad"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset('bhargavsdesai/laion_improved_aesthetics_6.5plus_with_images', split='train[:100000]')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:00:27.350057Z","iopub.execute_input":"2022-12-15T00:00:27.350569Z","iopub.status.idle":"2022-12-15T00:43:30.223558Z","shell.execute_reply.started":"2022-12-15T00:00:27.350526Z","shell.execute_reply":"2022-12-15T00:43:30.221285Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset parquet/bhargavsdesai--laion_improved_aesthetics_6.5plus_with_images to /root/.cache/huggingface/datasets/bhargavsdesai___parquet/bhargavsdesai--laion_improved_aesthetics_6.5plus_with_images-d76ebc4bf0ed162f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823cf30687dd42e393bcaa8df470edce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/530M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1caf81c243f5439b9f9dd2bc859e7bf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/534M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81006aababb43e2a598cdc29920b69a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39c5e084d59474bb7ff91eadd8dc93e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/592M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a49e0579bcf4235a4fb51c9b2768fc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b097c02cc64274958755913d5704fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/544M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0454bc743bcb43589ff4f363e3490850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/531M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bde73272136432689c8230baa71838f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/537M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cddfe2cd4e1843d887452bb3c8513c4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/533M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a81069f6a4e4a71a48c7aba1a0067ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/524M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14038c1860324cc2a27b2a174500c445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9650bcb0e0d46a69489023c5b47f63d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/568M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f3b8ddf228640b1897deb0e50b82607"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/565M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e9c6976e4a4de1acfa9a264836a274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/554M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7d5e99df854a68b1a04bd89524df6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/537M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54fe4e83bd064577b833ff34f667773e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/505M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"636e0e614bfc40498d7c5edd5e798540"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/532M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c610aacaa084ad59a00cc0cc4a98575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/530M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075958928a164ffeaf09a918b9193280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/505M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90714f8c20b343e2907e7b45ee7e2f1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7f2fc2727a04a619a059bd414e9db39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/485M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbf53b14d11e4841ba2c22d989446e08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d02f08cdb0440d092f77e1c8897729a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be8132696bd043ec96ba57ff139d99ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/494M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c2263df6b754323bb8c48629250a862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/479M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c4c50b2d9dc4bbb81a7ca3a620bf00d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/478M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f141726a8040f187f8e01fb9721b6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/522M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"670e175d2f4b4fd4b3a8efa7f2866f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/523M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea1ecdb4c674335a84299ee41356f7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/514M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a30e12d7a8ed40d885f5c423323a60a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322834aa9f7e4b1a9f6ded131092b4b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/514M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b599fc85af488b9bd16d366223d991"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/520M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb9b7333cc5f4fe39ac73c845d98ba20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/490M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d885661bff3e4dab8c314f69f7a1b917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/535M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bf5c517fde34c009f76a7a1681c976b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/528M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d539d53cce454bdba93b5e3797980edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/515M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf9331f2e2c40a7a01fad84718f9ca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f9b4414dc24164bed2782b78b074cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/489M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7fa9e4e9b744abcb5a7d38c4bd23b48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/495M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41119182a0524d62a55cc7a52c6d6acc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47bb87be30654bbe8cea9366e261a540"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/544M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ad4df0cd944e8aad9d8889b51d23fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/549M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2118339830ae44a899cbb901d883a454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/538M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f9d2d259774022ac7d1c6086ee3c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e1b08bf98e44c099c4d5328cd136143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/514M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e663e6482fc145f1a630ad8d5ddc240b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/536M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c088f16c824e6f9306414662a35987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/507M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c1449dd1d9484ea6693c62b8bb02e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/531M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd726f16b564f9bb26b9242c24ac37d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/509M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bfbf4454f254fe3a8a73e8e6f4fffee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"348d3f4ab5974ade8e67ba4b90054f17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/bhargavsdesai___parquet/bhargavsdesai--laion_improved_aesthetics_6.5plus_with_images-d76ebc4bf0ed162f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = dataset.cast_column('image', Image(decode=True))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:44:32.863346Z","iopub.execute_input":"2022-12-15T00:44:32.863733Z","iopub.status.idle":"2022-12-15T00:44:32.888531Z","shell.execute_reply.started":"2022-12-15T00:44:32.863677Z","shell.execute_reply":"2022-12-15T00:44:32.887666Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def tokenize_captions(examples, is_train=True):\n    captions = []\n    captions = [caption for caption in examples['text']]\n    inputs = tokenizer(captions, max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n    input_ids = inputs.input_ids\n    return input_ids\n\ntrain_transforms = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((128, 128), interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n        torchvision.transforms.RandomHorizontalFlip(),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize([0.5], [0.5]),\n    ]\n)\n\ndef preprocess_train(examples):\n    images = [image.convert(\"RGB\") for image in examples['image']]\n    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n    examples[\"input_ids\"] = tokenize_captions(examples)\n    return examples\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:44:35.836796Z","iopub.execute_input":"2022-12-15T00:44:35.837269Z","iopub.status.idle":"2022-12-15T00:44:35.852624Z","shell.execute_reply.started":"2022-12-15T00:44:35.837228Z","shell.execute_reply":"2022-12-15T00:44:35.851468Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset.with_transform(preprocess_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:44:39.154474Z","iopub.execute_input":"2022-12-15T00:44:39.154865Z","iopub.status.idle":"2022-12-15T00:44:40.719242Z","shell.execute_reply.started":"2022-12-15T00:44:39.154834Z","shell.execute_reply":"2022-12-15T00:44:40.718232Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n        input_ids = [example[\"input_ids\"] for example in examples]\n        padded_tokens = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": padded_tokens.input_ids,\n            \"attention_mask\": padded_tokens.attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:44:44.121440Z","iopub.execute_input":"2022-12-15T00:44:44.121816Z","iopub.status.idle":"2022-12-15T00:44:44.127929Z","shell.execute_reply.started":"2022-12-15T00:44:44.121780Z","shell.execute_reply":"2022-12-15T00:44:44.126918Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(\n    train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=16\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:44:45.688522Z","iopub.execute_input":"2022-12-15T00:44:45.689198Z","iopub.status.idle":"2022-12-15T00:44:45.693907Z","shell.execute_reply.started":"2022-12-15T00:44:45.689159Z","shell.execute_reply":"2022-12-15T00:44:45.692859Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4, weight_decay=1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:44:47.516897Z","iopub.execute_input":"2022-12-15T00:44:47.517244Z","iopub.status.idle":"2022-12-15T00:44:47.524775Z","shell.execute_reply.started":"2022-12-15T00:44:47.517213Z","shell.execute_reply":"2022-12-15T00:44:47.523714Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Scheduler and math around the number of training steps.\nnum_update_steps_per_epoch = len(train_dataloader) \nmax_train_steps = 1 * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler('cosine_with_restarts', optimizer=optimizer,num_warmup_steps=1000, num_training_steps=max_train_steps)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:45:13.141170Z","iopub.execute_input":"2022-12-15T00:45:13.141535Z","iopub.status.idle":"2022-12-15T00:45:13.149867Z","shell.execute_reply.started":"2022-12-15T00:45:13.141503Z","shell.execute_reply":"2022-12-15T00:45:13.147149Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"noise_scheduler = DDPMScheduler.from_pretrained('CompVis/stable-diffusion-v1-4', subfolder=\"scheduler\")","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:32:08.052796Z","iopub.execute_input":"2022-12-15T02:32:08.053747Z","iopub.status.idle":"2022-12-15T02:32:08.398176Z","shell.execute_reply.started":"2022-12-15T02:32:08.053694Z","shell.execute_reply":"2022-12-15T02:32:08.397242Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"accelerator = Accelerator()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:45:16.961268Z","iopub.execute_input":"2022-12-15T00:45:16.961627Z","iopub.status.idle":"2022-12-15T00:45:16.966838Z","shell.execute_reply.started":"2022-12-15T00:45:16.961597Z","shell.execute_reply":"2022-12-15T00:45:16.965712Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":" unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        unet, optimizer, train_dataloader, lr_scheduler\n    )\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:45:18.817920Z","iopub.execute_input":"2022-12-15T00:45:18.818292Z","iopub.status.idle":"2022-12-15T00:45:18.832626Z","shell.execute_reply.started":"2022-12-15T00:45:18.818260Z","shell.execute_reply":"2022-12-15T00:45:18.831729Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_batch_size = 16\nnum_train_epochs = 1\nmax_train_steps = num_train_epochs * len(train_dataloader)\nmax_grad_norm = 1\ngradient_accumulation_steps = 1\nuse_ema = False\npush_to_hub = True","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:13:24.011415Z","iopub.execute_input":"2022-12-15T02:13:24.011829Z","iopub.status.idle":"2022-12-15T02:13:24.021523Z","shell.execute_reply.started":"2022-12-15T02:13:24.011792Z","shell.execute_reply":"2022-12-15T02:13:24.020499Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import logging\nlogging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:45:32.801185Z","iopub.execute_input":"2022-12-15T00:45:32.801545Z","iopub.status.idle":"2022-12-15T00:45:32.806967Z","shell.execute_reply.started":"2022-12-15T00:45:32.801513Z","shell.execute_reply":"2022-12-15T00:45:32.805757Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"if accelerator.is_main_process:\n    accelerator.init_trackers(\"text2image-fine-tune\")","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:45:39.074475Z","iopub.execute_input":"2022-12-15T00:45:39.074862Z","iopub.status.idle":"2022-12-15T00:45:39.079608Z","shell.execute_reply.started":"2022-12-15T00:45:39.074828Z","shell.execute_reply":"2022-12-15T00:45:39.078597Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Train!\ntotal_batch_size = train_batch_size * accelerator.num_processes\n\nlogging.info(\"***** Running training *****\")\nlogging.info(f\"  Num examples = {len(train_dataset)}\")\nlogging.info(f\"  Num Epochs = {num_train_epochs}\")\nlogging.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\nlogging.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\nlogging.info(f\"  Total optimization steps = {max_train_steps}\")\n\n# Only show the progress bar once on each machine.\nprogress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\nprogress_bar.set_description(\"Steps\")\nglobal_step = 0\nlosess = list()\n\nfor epoch in range(num_train_epochs):\n    unet.train()\n    train_loss = 0.0\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(unet):\n            # Convert images to latent space\n            latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n            latents = latents * 0.18215\n\n            # Sample noise that we'll add to the latents\n            noise = torch.randn_like(latents)\n            bsz = latents.shape[0]\n            # Sample a random timestep for each image\n            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n            timesteps = timesteps.long()\n\n            # Add noise to the latents according to the noise magnitude at each timestep\n            # (this is the forward diffusion process)\n            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n            # Get the text embedding for conditioning\n            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n            # Get the target for loss depending on the prediction type\n            if noise_scheduler.config.prediction_type == \"epsilon\":\n                target = noise\n            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n            else:\n                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n            # Predict the noise residual and compute loss\n            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states)\n            loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n\n            # Gather the losses across all processes for logging (if we use distributed training).\n            avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n            train_loss += avg_loss.item() / gradient_accumulation_steps\n            losess.append(train_loss)\n\n            # Backpropagate\n            accelerator.backward(loss)\n            if accelerator.sync_gradients:\n                accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        # Checks if the accelerator has performed an optimization step behind the scenes\n        if accelerator.sync_gradients:\n            if use_ema:\n                ema_unet.step(unet.parameters())\n            progress_bar.update(1)\n            global_step += 1\n            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n            train_loss = 0.0\n\n        logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n        progress_bar.set_postfix(**logs)\n        \n        if global_step >= max_train_steps:\n            break\n\n# Create the pipeline using the trained modules and save it.\naccelerator.wait_for_everyone()\nif accelerator.is_main_process:\n    unet = accelerator.unwrap_model(unet)\n    if args.use_ema:\n        ema_unet.copy_to(unet.parameters())\n\n    pipeline = StableDiffusionPipeline.from_pretrained(\n        args.pretrained_model_name_or_path,\n        text_encoder=text_encoder,\n        vae=vae,\n        unet=unet,\n        revision=args.revision,\n    )\n    pipeline.save_pretrained(args.output_dir)\n    \n    if push_to_hub:\n        repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\n\naccelerator.end_training()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T00:47:23.330044Z","iopub.execute_input":"2022-12-15T00:47:23.330417Z","iopub.status.idle":"2022-12-15T02:13:11.971530Z","shell.execute_reply.started":"2022-12-15T00:47:23.330385Z","shell.execute_reply":"2022-12-15T02:13:11.967115Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6c68546e85341239cfb38247bca0003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_234/\u001b[0m\u001b[1;33m422877836.py\u001b[0m:\u001b[94m84\u001b[0m in \u001b[92m<module>\u001b[0m                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_234/422877836.py'\u001b[0m                           \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mNameError: \u001b[0mname \u001b[32m'args'\u001b[0m is not defined\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_234/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">422877836.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">84</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_234/422877836.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span> is not defined\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"working_dir='output_dir'","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:14:06.996076Z","iopub.execute_input":"2022-12-15T02:14:06.996435Z","iopub.status.idle":"2022-12-15T02:14:07.001813Z","shell.execute_reply.started":"2022-12-15T02:14:06.996404Z","shell.execute_reply":"2022-12-15T02:14:07.000704Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"torch.save(unet.state_dict(), 'laion100k')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:16:58.805991Z","iopub.execute_input":"2022-12-15T02:16:58.806428Z","iopub.status.idle":"2022-12-15T02:17:04.646747Z","shell.execute_reply.started":"2022-12-15T02:16:58.806390Z","shell.execute_reply":"2022-12-15T02:17:04.645714Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"torch_device = accelerator.device","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:21:26.916552Z","iopub.execute_input":"2022-12-15T02:21:26.917245Z","iopub.status.idle":"2022-12-15T02:21:26.922837Z","shell.execute_reply.started":"2022-12-15T02:21:26.917205Z","shell.execute_reply":"2022-12-15T02:21:26.921698Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"dataset[22421]['text']","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:38:35.139741Z","iopub.execute_input":"2022-12-15T02:38:35.140236Z","iopub.status.idle":"2022-12-15T02:38:35.219979Z","shell.execute_reply.started":"2022-12-15T02:38:35.140191Z","shell.execute_reply":"2022-12-15T02:38:35.218911Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"'Sunrise, morning, boundary range, meadows, alaska, british columbia, Summer, photo'"},"metadata":{}}]},{"cell_type":"code","source":"prompt = [\"Abstract\"]\nheight = 128                        # default height of Stable Diffusion\nwidth = 128                         # default width of Stable Diffusion\nnum_inference_steps = 50            # Number of denoising steps\nguidance_scale = 7.5                # Scale for classifier-free guidance\ngenerator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\nbatch_size = 1\n\n# Prep text \ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\n\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n# Prep Scheduler\nnoise_scheduler.set_timesteps(num_inference_steps)\n\n# Prep latents\nlatents = torch.randn(\n  (batch_size, 4, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\nlatents = latents * noise_scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n\n# Loop\nwith autocast(\"cuda\"):\n    for i, t in tqdm(enumerate(scheduler.timesteps)):\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n#         sigma = noise_scheduler.sigmas[i]\n        # Scale the latents (preconditioning):\n        # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below\n        latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n        # compute the previous noisy sample x_t -> x_t-1\n        # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n        latents = noise_scheduler.step(noise_pred, t.long(), latents).prev_sample\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n# Display\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\npil_images[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-15T02:43:32.937385Z","iopub.execute_input":"2022-12-15T02:43:32.937794Z","iopub.status.idle":"2022-12-15T02:43:35.775174Z","shell.execute_reply.started":"2022-12-15T02:43:32.937760Z","shell.execute_reply":"2022-12-15T02:43:35.774157Z"},"trusted":true},"execution_count":107,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c583bf41f9af43f3b3bdafb28761cbae"}},"metadata":{}},{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=128x128>","image/png":"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\n"},"metadata":{}}]}]}